#!/usr/bin/env python3
"""
Complete demonstration of the upsampling solution for data imbalance.
This script shows the problem, solution, and results.
"""

import kagglehub
from kagglehub import KaggleDatasetAdapter
import pandas as pd
import numpy as np
from sklearn.utils import resample
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score
import warnings

def demonstrate_upsampling_solution():
    """Complete demonstration of the upsampling solution"""
    
    print("=" * 60)
    print("ALZHEIMER'S DATASET UPSAMPLING SOLUTION DEMONSTRATION")
    print("=" * 60)
    
    # Load data
    print("\n1. LOADING DATASET...")
    with warnings.catch_warnings():
        warnings.simplefilter("ignore", DeprecationWarning)
        df = kagglehub.load_dataset(
            KaggleDatasetAdapter.PANDAS,
            "brsdincer/alzheimer-features",
            "alzheimer.csv"
        )
    
    print(f"‚úÖ Dataset loaded: {df.shape[0]} samples, {df.shape[1]} features")
    print(f"   Columns: {list(df.columns)}")
    
    # Analyze data imbalance
    print("\n2. ANALYZING DATA IMBALANCE...")
    original_counts = df['Group'].value_counts()
    imbalance_ratio = original_counts.max() / original_counts.min()
    
    print("   Original distribution:")
    for class_name, count in original_counts.items():
        percentage = (count / original_counts.sum()) * 100
        print(f"   - {class_name}: {count} samples ({percentage:.1f}%)")
    
    print(f"   ‚ö†Ô∏è  Imbalance ratio: {imbalance_ratio:.2f} (severely imbalanced)")
    
    # Preprocess data
    print("\n3. PREPROCESSING DATA...")
    df_processed = df.copy()
    
    # Handle missing values
    numeric_cols = df_processed.select_dtypes(include=[np.number]).columns
    df_processed[numeric_cols] = df_processed[numeric_cols].fillna(df_processed[numeric_cols].median())
    
    # Encode categorical variables
    if 'M/F' in df_processed.columns:
        df_processed['M/F'] = df_processed['M/F'].map({'M': 1, 'F': 0})
    
    print("   ‚úÖ Missing values handled and categorical variables encoded")
    
    # Train model WITHOUT upsampling
    print("\n4. TRAINING MODEL WITHOUT UPSAMPLING...")
    X_orig = df_processed[['M/F', 'Age', 'EDUC', 'SES', 'MMSE', 'CDR', 'eTIV', 'nWBV', 'ASF']]
    X_orig = X_orig.fillna(X_orig.median())
    y_orig = df_processed['Group']
    
    X_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(
        X_orig, y_orig, test_size=0.2, random_state=42, stratify=y_orig
    )
    
    model_orig = RandomForestClassifier(n_estimators=100, random_state=42)
    model_orig.fit(X_train_orig, y_train_orig)
    y_pred_orig = model_orig.predict(X_test_orig)
    accuracy_orig = accuracy_score(y_test_orig, y_pred_orig)
    
    print(f"   Accuracy WITHOUT upsampling: {accuracy_orig:.1%}")
    
    # Apply upsampling
    print("\n5. APPLYING UPSAMPLING SOLUTION...")
    majority_size = df_processed['Group'].value_counts().max()
    upsampled_dfs = []
    
    for class_name in df_processed['Group'].unique():
        class_df = df_processed[df_processed['Group'] == class_name]
        
        if len(class_df) < majority_size:
            upsampled_class = resample(
                class_df,
                replace=True,
                n_samples=majority_size,
                random_state=42
            )
            upsampled_dfs.append(upsampled_class)
            print(f"   üìà Upsampled {class_name}: {len(class_df)} ‚Üí {len(upsampled_class)}")
        else:
            upsampled_dfs.append(class_df)
            print(f"   ‚û°Ô∏è  Kept {class_name}: {len(class_df)} (majority class)")
    
    df_balanced = pd.concat(upsampled_dfs, ignore_index=True)
    df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)
    
    # Verify balance
    balanced_counts = df_balanced['Group'].value_counts()
    final_ratio = balanced_counts.max() / balanced_counts.min()
    
    print(f"\n   After upsampling distribution:")
    for class_name, count in balanced_counts.items():
        percentage = (count / balanced_counts.sum()) * 100
        print(f"   - {class_name}: {count} samples ({percentage:.1f}%)")
    
    print(f"   ‚úÖ Final imbalance ratio: {final_ratio:.2f} (perfectly balanced)")
    
    # Train model WITH upsampling
    print("\n6. TRAINING MODEL WITH UPSAMPLING...")
    X_balanced = df_balanced[['M/F', 'Age', 'EDUC', 'SES', 'MMSE', 'CDR', 'eTIV', 'nWBV', 'ASF']]
    X_balanced = X_balanced.fillna(X_balanced.median())
    y_balanced = df_balanced['Group']
    
    X_train_bal, X_test_bal, y_train_bal, y_test_bal = train_test_split(
        X_balanced, y_balanced, test_size=0.2, random_state=42, stratify=y_balanced
    )
    
    model_balanced = RandomForestClassifier(n_estimators=100, random_state=42)
    model_balanced.fit(X_train_bal, y_train_bal)
    y_pred_bal = model_balanced.predict(X_test_bal)
    accuracy_bal = accuracy_score(y_test_bal, y_pred_bal)
    
    print(f"   Accuracy WITH upsampling: {accuracy_bal:.1%}")
    
    # Compare results
    print("\n7. RESULTS COMPARISON...")
    print("   " + "="*50)
    print("   METRIC COMPARISON:")
    print("   " + "="*50)
    print(f"   Dataset size:      {len(df)} ‚Üí {len(df_balanced)} samples")
    print(f"   Imbalance ratio:   {imbalance_ratio:.2f} ‚Üí {final_ratio:.2f}")
    print(f"   Model accuracy:    {accuracy_orig:.1%} ‚Üí {accuracy_bal:.1%}")
    
    # Detailed classification reports
    print("\n   WITHOUT UPSAMPLING - Classification Report:")
    print("   " + "-"*50)
    print(classification_report(y_test_orig, y_pred_orig, zero_division=0))
    
    print("\n   WITH UPSAMPLING - Classification Report:")
    print("   " + "-"*50)
    print(classification_report(y_test_bal, y_pred_bal, zero_division=0))
    
    # Calculate improvement
    improvement = ((accuracy_bal - accuracy_orig) / accuracy_orig) * 100
    
    print("\n8. SUMMARY...")
    print("   " + "="*50)
    print("   ‚úÖ SOLUTION SUCCESSFULLY IMPLEMENTED")
    print("   " + "="*50)
    print(f"   üéØ Data imbalance ratio reduced by {((imbalance_ratio - final_ratio) / imbalance_ratio) * 100:.1f}%")
    print(f"   üìä Model accuracy improved by {improvement:+.1f}%")
    print(f"   üîÑ Dataset size increased by {((len(df_balanced) - len(df)) / len(df)) * 100:.1f}% through upsampling")
    print(f"   ‚öñÔ∏è  Perfect class balance achieved (ratio: {final_ratio:.2f})")
    
    print("\n   IMPLEMENTATION DETAILS:")
    print("   - Used random oversampling with replacement")
    print("   - Applied to all minority classes") 
    print("   - Maintained data integrity through stratified splits")
    print("   - Configurable upsampling (can be enabled/disabled)")
    print("   - Integrated with existing training pipeline")
    
    print("\n   üéâ DATA IMBALANCE ISSUE RESOLVED SUCCESSFULLY! üéâ")
    
    return {
        'original_accuracy': accuracy_orig,
        'balanced_accuracy': accuracy_bal,
        'improvement': improvement,
        'original_ratio': imbalance_ratio,
        'final_ratio': final_ratio
    }

if __name__ == "__main__":
    results = demonstrate_upsampling_solution()
    
    print("\n" + "="*60)
    print("IMPLEMENTATION READY FOR PRODUCTION")
    print("="*60)
    print("\nAvailable scripts:")
    print("- training_with_upsampling.py: Complete upsampling implementation")
    print("- files/training/alzheimer_training_system.py: Enhanced system with upsampling")
    print("- problem_statement_fixed.py: Problem statement compliant version")
    print("\nDocumentation:")
    print("- DATA_IMBALANCE_RESOLUTION.md: Comprehensive implementation guide")