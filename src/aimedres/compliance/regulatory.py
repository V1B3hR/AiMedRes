#!/usr/bin/env python3
"""
Regulatory Compliance Module for Clinical Decision Support System

Provides comprehensive compliance frameworks for medical AI systems:
- HIPAA compliance and audit logging
- FDA validation pathway support
- Clinical trial data management
- Regulatory reporting dashboards
- Quality assurance workflows
- Risk management protocols

This module ensures adherence to healthcare regulatory requirements.
"""

import json
import logging
import hashlib
from typing import Dict, List, Any, Optional, Tuple, Union
from datetime import datetime, timedelta
from dataclasses import dataclass, asdict
from enum import Enum
from pathlib import Path
import sqlite3
import uuid
from abc import ABC, abstractmethod

# Import existing security components
from secure_medical_processor import SecureMedicalDataProcessor

logger = logging.getLogger("RegulatoryCompliance")


class ComplianceFramework(Enum):
    """Supported regulatory compliance frameworks"""
    HIPAA = "HIPAA"
    FDA_510K = "FDA_510K"
    FDA_DE_NOVO = "FDA_DE_NOVO"
    GDPR = "GDPR"
    ISO_13485 = "ISO_13485"
    ISO_14155 = "ISO_14155"  # Clinical investigation of medical devices
    ICH_GCP = "ICH_GCP"     # Good Clinical Practice


class AuditEventType(Enum):
    """Types of audit events for compliance tracking"""
    DATA_ACCESS = "data_access"
    MODEL_PREDICTION = "model_prediction"
    DATA_EXPORT = "data_export"
    SYSTEM_LOGIN = "system_login"
    CONFIGURATION_CHANGE = "configuration_change"
    MODEL_TRAINING = "model_training"
    VALIDATION_TEST = "validation_test"
    ADVERSE_EVENT = "adverse_event"
    SECURITY_INCIDENT = "security_incident"


@dataclass
class AuditEvent:
    """Audit event record structure"""
    event_id: str
    event_type: AuditEventType
    timestamp: datetime
    user_id: Optional[str]
    patient_id: Optional[str]
    resource_accessed: str
    action_performed: str
    outcome: str  # 'SUCCESS', 'FAILURE', 'WARNING'
    ip_address: Optional[str]
    user_agent: Optional[str]
    additional_data: Dict[str, Any]
    risk_level: str = 'LOW'  # 'LOW', 'MEDIUM', 'HIGH', 'CRITICAL'


@dataclass
class ValidationRecord:
    """Model validation record for FDA compliance"""
    validation_id: str
    model_version: str
    validation_type: str  # 'analytical', 'clinical', 'usability'
    test_dataset: str
    performance_metrics: Dict[str, float]
    validation_date: datetime
    validator: str
    clinical_endpoints: List[str]
    success_criteria: Dict[str, Any]
    results: Dict[str, Any]
    status: str  # 'PENDING', 'PASSED', 'FAILED', 'CONDITIONAL'
    regulatory_notes: str


@dataclass
class AdverseEvent:
    """Adverse event record for safety monitoring"""
    event_id: str
    patient_id: str
    event_date: datetime
    description: str
    severity: str  # 'MILD', 'MODERATE', 'SEVERE'
    causality: str  # 'UNRELATED', 'POSSIBLE', 'PROBABLE', 'DEFINITE'
    outcome: str   # 'RECOVERED', 'ONGOING', 'UNKNOWN'
    ai_system_involved: bool
    model_version: Optional[str]
    prediction_confidence: Optional[float]
    clinical_context: Dict[str, Any]
    reporter: str
    follow_up_required: bool


class HIPAAComplianceManager:
    """
    HIPAA compliance management for protected health information.
    """
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.audit_db_path = config.get('audit_db_path', 'compliance_audit.db')
        self._init_audit_database()
        
    def _init_audit_database(self):
        """Initialize audit database schema"""
        
        conn = sqlite3.connect(self.audit_db_path)
        cursor = conn.cursor()
        
        # Audit events table
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS audit_events (
                event_id TEXT PRIMARY KEY,
                event_type TEXT NOT NULL,
                timestamp TEXT NOT NULL,
                user_id TEXT,
                patient_id TEXT,
                resource_accessed TEXT NOT NULL,
                action_performed TEXT NOT NULL,
                outcome TEXT NOT NULL,
                ip_address TEXT,
                user_agent TEXT,
                additional_data TEXT,
                risk_level TEXT DEFAULT 'LOW'
            )
        ''')
        
        # PHI access log
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS phi_access_log (
                access_id TEXT PRIMARY KEY,
                timestamp TEXT NOT NULL,
                user_id TEXT NOT NULL,
                patient_id TEXT NOT NULL,
                data_elements TEXT NOT NULL,
                purpose TEXT NOT NULL,
                authorization_basis TEXT NOT NULL,
                minimum_necessary BOOLEAN DEFAULT TRUE,
                retention_period TEXT
            )
        ''')
        
        # Breach incidents
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS breach_incidents (
                incident_id TEXT PRIMARY KEY,
                discovery_date TEXT NOT NULL,
                incident_date TEXT NOT NULL,
                affected_individuals INTEGER NOT NULL,
                types_of_phi TEXT NOT NULL,
                description TEXT NOT NULL,
                containment_actions TEXT,
                notification_status TEXT DEFAULT 'PENDING',
                regulatory_reporting_required BOOLEAN DEFAULT FALSE
            )
        ''')
        
        conn.commit()
        conn.close()
    
    def log_audit_event(self, event: AuditEvent):
        """Log audit event for HIPAA compliance"""
        
        conn = sqlite3.connect(self.audit_db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            INSERT INTO audit_events (
                event_id, event_type, timestamp, user_id, patient_id,
                resource_accessed, action_performed, outcome, ip_address,
                user_agent, additional_data, risk_level
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        ''', (
            event.event_id,
            event.event_type.value,
            event.timestamp.isoformat(),
            event.user_id,
            event.patient_id,
            event.resource_accessed,
            event.action_performed,
            event.outcome,
            event.ip_address,
            event.user_agent,
            json.dumps(event.additional_data),
            event.risk_level
        ))
        
        conn.commit()
        conn.close()
        
        # Check for high-risk events
        if event.risk_level in ['HIGH', 'CRITICAL']:
            self._trigger_security_alert(event)
    
    def log_phi_access(self, user_id: str, patient_id: str, 
                      data_elements: List[str], purpose: str,
                      authorization_basis: str = 'treatment'):
        """Log PHI access for minimum necessary compliance"""
        
        access_id = str(uuid.uuid4())
        
        conn = sqlite3.connect(self.audit_db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            INSERT INTO phi_access_log (
                access_id, timestamp, user_id, patient_id, data_elements,
                purpose, authorization_basis, minimum_necessary, retention_period
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
        ''', (
            access_id,
            datetime.now().isoformat(),
            user_id,
            patient_id,
            json.dumps(data_elements),
            purpose,
            authorization_basis,
            True,  # Assume minimum necessary by default
            '6 years'  # Standard HIPAA retention period
        ))
        
        conn.commit()
        conn.close()
        
        return access_id
    
    def check_data_minimization(self, requested_data: Dict[str, Any], 
                               purpose: str) -> Dict[str, Any]:
        """Check and enforce data minimization principles"""
        
        # Define minimum necessary data sets for different purposes
        minimal_datasets = {
            'risk_assessment': [
                'patient_id', 'Age', 'M/F', 'MMSE', 'CDR', 'EDUC'
            ],
            'treatment_planning': [
                'patient_id', 'Age', 'M/F', 'MMSE', 'CDR', 'current_medications'
            ],
            'research': [
                'Age', 'M/F', 'MMSE', 'CDR', 'EDUC'  # No patient_id for research
            ]
        }
        
        required_fields = minimal_datasets.get(purpose, list(requested_data.keys()))
        
        # Filter to minimum necessary
        minimized_data = {
            key: value for key, value in requested_data.items()
            if key in required_fields
        }
        
        # Log data minimization decision
        audit_event = AuditEvent(
            event_id=str(uuid.uuid4()),
            event_type=AuditEventType.DATA_ACCESS,
            timestamp=datetime.now(),
            user_id=None,
            patient_id=requested_data.get('patient_id'),
            resource_accessed='patient_data',
            action_performed='data_minimization_check',
            outcome='SUCCESS',
            ip_address=None,
            user_agent=None,
            additional_data={
                'original_fields': list(requested_data.keys()),
                'minimized_fields': list(minimized_data.keys()),
                'purpose': purpose
            }
        )
        
        self.log_audit_event(audit_event)
        
        return minimized_data
    
    def generate_compliance_report(self, start_date: datetime, 
                                 end_date: datetime) -> Dict[str, Any]:
        """Generate HIPAA compliance report"""
        
        conn = sqlite3.connect(self.audit_db_path)
        cursor = conn.cursor()
        
        # Get audit events in date range
        cursor.execute('''
            SELECT event_type, outcome, risk_level, COUNT(*) as count
            FROM audit_events
            WHERE timestamp BETWEEN ? AND ?
            GROUP BY event_type, outcome, risk_level
        ''', (start_date.isoformat(), end_date.isoformat()))
        
        audit_summary = cursor.fetchall()
        
        # Get PHI access statistics
        cursor.execute('''
            SELECT authorization_basis, COUNT(*) as count
            FROM phi_access_log
            WHERE timestamp BETWEEN ? AND ?
            GROUP BY authorization_basis
        ''', (start_date.isoformat(), end_date.isoformat()))
        
        phi_access_summary = cursor.fetchall()
        
        # Get breach incidents
        cursor.execute('''
            SELECT * FROM breach_incidents
            WHERE incident_date BETWEEN ? AND ?
        ''', (start_date.isoformat(), end_date.isoformat()))
        
        breach_incidents = cursor.fetchall()
        
        conn.close()
        
        return {
            'report_period': {
                'start_date': start_date.isoformat(),
                'end_date': end_date.isoformat()
            },
            'audit_summary': [
                {
                    'event_type': row[0],
                    'outcome': row[1],
                    'risk_level': row[2],
                    'count': row[3]
                }
                for row in audit_summary
            ],
            'phi_access_summary': [
                {
                    'authorization_basis': row[0],
                    'count': row[1]
                }
                for row in phi_access_summary
            ],
            'breach_incidents': len(breach_incidents),
            'compliance_status': 'COMPLIANT' if len(breach_incidents) == 0 else 'UNDER_REVIEW'
        }
    
    def _trigger_security_alert(self, event: AuditEvent):
        """Trigger security alert for high-risk events"""
        
        alert_data = {
            'alert_id': str(uuid.uuid4()),
            'timestamp': datetime.now().isoformat(),
            'event_id': event.event_id,
            'risk_level': event.risk_level,
            'description': f"High-risk event detected: {event.action_performed}",
            'requires_investigation': True
        }
        
        logger.warning(f"Security alert triggered: {alert_data}")
        
        # In production, this would send alerts to security team


class FDAValidationManager:
    """
    FDA validation pathway management for medical AI systems.
    """
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.validation_db_path = config.get('validation_db_path', 'fda_validation.db')
        self._init_validation_database()
        
    def _init_validation_database(self):
        """Initialize FDA validation database schema"""
        
        conn = sqlite3.connect(self.validation_db_path)
        cursor = conn.cursor()
        
        # Validation records table
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS validation_records (
                validation_id TEXT PRIMARY KEY,
                model_version TEXT NOT NULL,
                validation_type TEXT NOT NULL,
                test_dataset TEXT NOT NULL,
                performance_metrics TEXT NOT NULL,
                validation_date TEXT NOT NULL,
                validator TEXT NOT NULL,
                clinical_endpoints TEXT NOT NULL,
                success_criteria TEXT NOT NULL,
                results TEXT NOT NULL,
                status TEXT NOT NULL,
                regulatory_notes TEXT
            )
        ''')
        
        # Adverse events table
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS adverse_events (
                event_id TEXT PRIMARY KEY,
                patient_id TEXT NOT NULL,
                event_date TEXT NOT NULL,
                description TEXT NOT NULL,
                severity TEXT NOT NULL,
                causality TEXT NOT NULL,
                outcome TEXT NOT NULL,
                ai_system_involved BOOLEAN NOT NULL,
                model_version TEXT,
                prediction_confidence REAL,
                clinical_context TEXT NOT NULL,
                reporter TEXT NOT NULL,
                follow_up_required BOOLEAN DEFAULT FALSE
            )
        ''')
        
        # Clinical performance tracking
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS clinical_performance (
                record_id TEXT PRIMARY KEY,
                model_version TEXT NOT NULL,
                timestamp TEXT NOT NULL,
                patient_count INTEGER NOT NULL,
                true_positives INTEGER NOT NULL,
                false_positives INTEGER NOT NULL,
                true_negatives INTEGER NOT NULL,
                false_negatives INTEGER NOT NULL,
                sensitivity REAL NOT NULL,
                specificity REAL NOT NULL,
                ppv REAL NOT NULL,
                npv REAL NOT NULL
            )
        ''')
        
        conn.commit()
        conn.close()
    
    def create_validation_protocol(self, model_version: str, 
                                 submission_type: str = '510k') -> Dict[str, Any]:
        """Create FDA validation protocol"""
        
        protocol_templates = {
            '510k': {
                'analytical_validation': [
                    'Algorithm performance testing',
                    'Robustness testing',
                    'Cybersecurity analysis',
                    'Software verification'
                ],
                'clinical_validation': [
                    'Clinical accuracy study',
                    'Clinical utility demonstration',
                    'User interface validation'
                ],
                'required_endpoints': [
                    'Sensitivity ≥ 90%',
                    'Specificity ≥ 85%',
                    'PPV ≥ 80%',
                    'NPV ≥ 90%'
                ]
            },
            'de_novo': {
                'analytical_validation': [
                    'Comprehensive algorithm validation',
                    'Multi-site testing',
                    'Edge case analysis',
                    'Bias evaluation'
                ],
                'clinical_validation': [
                    'Pivotal clinical study',
                    'Real-world evidence',
                    'Long-term safety monitoring'
                ],
                'required_endpoints': [
                    'Primary effectiveness endpoint',
                    'Safety endpoints',
                    'Quality of life measures'
                ]
            }
        }
        
        template = protocol_templates.get(submission_type.lower(), protocol_templates['510k'])
        
        protocol = {
            'protocol_id': str(uuid.uuid4()),
            'model_version': model_version,
            'submission_type': submission_type,
            'created_date': datetime.now().isoformat(),
            'validation_phases': template,
            'status': 'DRAFT',
            'regulatory_pathway': self._determine_regulatory_pathway(model_version)
        }
        
        return protocol
    
    def record_validation_result(self, validation_record: ValidationRecord):
        """Record validation test results"""
        
        conn = sqlite3.connect(self.validation_db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            INSERT INTO validation_records (
                validation_id, model_version, validation_type, test_dataset,
                performance_metrics, validation_date, validator, clinical_endpoints,
                success_criteria, results, status, regulatory_notes
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        ''', (
            validation_record.validation_id,
            validation_record.model_version,
            validation_record.validation_type,
            validation_record.test_dataset,
            json.dumps(validation_record.performance_metrics),
            validation_record.validation_date.isoformat(),
            validation_record.validator,
            json.dumps(validation_record.clinical_endpoints),
            json.dumps(validation_record.success_criteria),
            json.dumps(validation_record.results),
            validation_record.status,
            validation_record.regulatory_notes
        ))
        
        conn.commit()
        conn.close()
        
        # Check if validation meets FDA requirements
        self._evaluate_validation_compliance(validation_record)
    
    def record_adverse_event(self, adverse_event: AdverseEvent):
        """Record adverse event for FDA safety monitoring"""
        
        conn = sqlite3.connect(self.validation_db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            INSERT INTO adverse_events (
                event_id, patient_id, event_date, description, severity,
                causality, outcome, ai_system_involved, model_version,
                prediction_confidence, clinical_context, reporter, follow_up_required
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        ''', (
            adverse_event.event_id,
            adverse_event.patient_id,
            adverse_event.event_date.isoformat(),
            adverse_event.description,
            adverse_event.severity,
            adverse_event.causality,
            adverse_event.outcome,
            adverse_event.ai_system_involved,
            adverse_event.model_version,
            adverse_event.prediction_confidence,
            json.dumps(adverse_event.clinical_context),
            adverse_event.reporter,
            adverse_event.follow_up_required
        ))
        
        conn.commit()
        conn.close()
        
        # Check if reportable to FDA
        self._assess_fda_reporting_requirement(adverse_event)
    
    def track_clinical_performance(self, model_version: str, 
                                 performance_data: Dict[str, Any]):
        """Track ongoing clinical performance for post-market surveillance"""
        
        # Calculate performance metrics
        tp = performance_data['true_positives']
        fp = performance_data['false_positives']
        tn = performance_data['true_negatives']
        fn = performance_data['false_negatives']
        
        sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0
        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
        ppv = tp / (tp + fp) if (tp + fp) > 0 else 0
        npv = tn / (tn + fn) if (tn + fn) > 0 else 0
        
        conn = sqlite3.connect(self.validation_db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            INSERT INTO clinical_performance (
                record_id, model_version, timestamp, patient_count,
                true_positives, false_positives, true_negatives, false_negatives,
                sensitivity, specificity, ppv, npv
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        ''', (
            str(uuid.uuid4()),
            model_version,
            datetime.now().isoformat(),
            tp + fp + tn + fn,
            tp, fp, tn, fn,
            sensitivity, specificity, ppv, npv
        ))
        
        conn.commit()
        conn.close()
        
        # Check for performance degradation
        self._monitor_performance_drift(model_version, sensitivity, specificity)
    
    def generate_fda_submission_package(self, model_version: str) -> Dict[str, Any]:
        """Generate comprehensive FDA submission package documentation"""
        
        conn = sqlite3.connect(self.validation_db_path)
        cursor = conn.cursor()
        
        # Get all validation records for this model
        cursor.execute('''
            SELECT * FROM validation_records WHERE model_version = ?
        ''', (model_version,))
        
        validation_records = cursor.fetchall()
        
        # Get clinical performance data
        cursor.execute('''
            SELECT * FROM clinical_performance WHERE model_version = ?
            ORDER BY timestamp DESC LIMIT 10
        ''', (model_version,))
        
        performance_records = cursor.fetchall()
        
        # Get adverse events
        cursor.execute('''
            SELECT * FROM adverse_events WHERE model_version = ?
        ''', (model_version,))
        
        adverse_events = cursor.fetchall()
        
        conn.close()
        
        # Generate comprehensive submission package
        submission_package = {
            'model_version': model_version,
            'submission_date': datetime.now().isoformat(),
            'regulatory_pathway': self._determine_regulatory_pathway(model_version),
            
            # Core validation data
            'validation_summary': {
                'total_validations': len(validation_records),
                'passed_validations': len([r for r in validation_records if r[10] == 'PASSED']),
                'validation_types': list(set([r[2] for r in validation_records]))
            },
            'clinical_performance': {
                'total_patients': sum([r[3] for r in performance_records]),
                'average_sensitivity': sum([r[8] for r in performance_records]) / len(performance_records) if performance_records else 0,
                'average_specificity': sum([r[9] for r in performance_records]) / len(performance_records) if performance_records else 0
            },
            'safety_profile': {
                'total_adverse_events': len(adverse_events),
                'serious_adverse_events': len([ae for ae in adverse_events if ae[6] == 'SERIOUS']),
                'resolved_events': len([ae for ae in adverse_events if ae[8] is not None])
            },
            
            # Enhanced FDA-specific sections
            'device_description': self._generate_device_description(model_version),
            'intended_use_statement': self._generate_intended_use_statement(model_version),
            'predicate_device_comparison': self._generate_predicate_comparison(model_version),
            'software_documentation': self._generate_software_documentation(model_version),
            'clinical_validation_report': self._generate_clinical_validation_report(validation_records, performance_records),
            'labeling_information': self._generate_labeling_information(model_version),
            
            # Assessment and readiness sections
            'quality_metrics': self._calculate_quality_metrics(validation_records),
            'risk_assessment': self._generate_risk_assessment(validation_records, adverse_events),
            'submission_readiness_score': self._calculate_submission_readiness_score(validation_records, performance_records, adverse_events),
            'pre_submission_checklist': self._generate_pre_submission_checklist(validation_records, performance_records, adverse_events),
            
            # Pre-submission meeting preparation
            'pre_submission_meeting_package': self._prepare_pre_submission_meeting_materials(model_version, validation_records, performance_records, adverse_events)
        }
        
        logger.info(f"Generated comprehensive FDA submission package for model {model_version}")
        return submission_package
    
    def _calculate_quality_metrics(self, validation_records: List) -> Dict[str, Any]:
        """Calculate quality metrics for FDA submission."""
        if not validation_records:
            return {
                'validation_completion_rate': 0,
                'average_accuracy': 0,
                'consistency_score': 0,
                'total_test_cases': 0,
                'passed_test_cases': 0,
                'failed_test_cases': 0
            }
        
        passed_validations = sum(1 for r in validation_records if r[10] == 'PASSED')
        total_validations = len(validation_records)
        
        # Calculate average accuracy from performance metrics (stored as JSON)
        avg_accuracy = 0
        accuracy_count = 0
        
        for record in validation_records:
            try:
                # Performance metrics are stored as JSON string in column 4
                if record[4]:  # Check if performance metrics exist
                    import json
                    metrics = json.loads(record[4]) if isinstance(record[4], str) else record[4]
                    if isinstance(metrics, dict) and 'sensitivity' in metrics:
                        # Use sensitivity as a proxy for accuracy if available
                        avg_accuracy += float(metrics['sensitivity'])
                        accuracy_count += 1
            except (json.JSONDecodeError, ValueError, TypeError):
                # Skip if metrics can't be parsed
                continue
        
        if accuracy_count > 0:
            avg_accuracy = avg_accuracy / accuracy_count
        
        return {
            'validation_completion_rate': (passed_validations / total_validations * 100) if total_validations > 0 else 0,
            'average_accuracy': avg_accuracy * 100,  # Convert to percentage
            'consistency_score': min(95.0, avg_accuracy * 105),  # Derived consistency metric
            'total_test_cases': total_validations,
            'passed_test_cases': passed_validations,
            'failed_test_cases': total_validations - passed_validations
        }
    
    def _generate_risk_assessment(self, validation_records: List, adverse_events: List) -> Dict[str, Any]:
        """Generate comprehensive risk assessment for FDA submission."""
        risk_factors = []
        risk_level = "LOW"
        
        # Assess validation failure rate
        if validation_records:
            failure_rate = sum(1 for r in validation_records if r[10] == 'FAILED') / len(validation_records)
            if failure_rate > 0.1:  # More than 10% failure rate
                risk_factors.append("High validation failure rate")
                risk_level = "MEDIUM"
            if failure_rate > 0.2:  # More than 20% failure rate
                risk_level = "HIGH"
        
        # Assess adverse events
        serious_events = len([ae for ae in adverse_events if ae[6] == 'SERIOUS'])
        if serious_events > 0:
            risk_factors.append(f"{serious_events} serious adverse events reported")
            risk_level = "HIGH" if serious_events > 5 else "MEDIUM"
        
        # Calculate overall risk score
        base_score = 100
        for _ in risk_factors:
            base_score -= 15
        
        risk_score = max(0, min(100, base_score))
        
        return {
            'risk_level': risk_level,
            'risk_score': risk_score,
            'risk_factors': risk_factors,
            'mitigation_strategies': self._get_mitigation_strategies(risk_factors),
            'regulatory_concerns': self._identify_regulatory_concerns(validation_records, adverse_events)
        }
    
    def _get_mitigation_strategies(self, risk_factors: List[str]) -> List[str]:
        """Get mitigation strategies for identified risk factors."""
        strategies = []
        
        if any('failure rate' in factor for factor in risk_factors):
            strategies.extend([
                'Implement additional validation testing',
                'Enhance model training with diverse datasets',
                'Establish stricter quality control processes'
            ])
        
        if any('adverse events' in factor for factor in risk_factors):
            strategies.extend([
                'Implement enhanced safety monitoring',
                'Establish rapid response protocols',
                'Increase physician oversight requirements'
            ])
        
        return strategies
    
    def _identify_regulatory_concerns(self, validation_records: List, adverse_events: List) -> List[str]:
        """Identify potential regulatory concerns."""
        concerns = []
        
        if not validation_records:
            concerns.append("Insufficient validation data for regulatory review")
        
        unresolved_events = [ae for ae in adverse_events if ae[8] is None]
        if unresolved_events:
            concerns.append(f"{len(unresolved_events)} unresolved adverse events")
        
        return concerns
    
    def _calculate_submission_readiness_score(self, validation_records: List, performance_records: List, adverse_events: List) -> Dict[str, Any]:
        """Calculate comprehensive FDA submission readiness score."""
        score_components = {
            'documentation_completeness': 0,
            'validation_completeness': 0,
            'performance_evidence': 0,
            'safety_documentation': 0,
            'quality_assurance': 0,
            'regulatory_compliance': 0
        }
        
        # Documentation completeness (0-20 points)
        required_docs = ['device_description', 'intended_use', 'predicate_comparison', 'software_docs', 'labeling']
        score_components['documentation_completeness'] = len(required_docs) * 4  # Assume all docs are generated
        
        # Validation completeness (0-20 points)
        if validation_records:
            validation_types = set([r[2] for r in validation_records])
            required_types = {'analytical', 'clinical', 'usability'}
            completion_rate = len(validation_types.intersection(required_types)) / len(required_types)
            passed_rate = sum(1 for r in validation_records if r[10] == 'PASSED') / len(validation_records)
            score_components['validation_completeness'] = completion_rate * passed_rate * 20
        
        # Performance evidence (0-20 points)
        if performance_records:
            # Higher score for more comprehensive performance data
            score_components['performance_evidence'] = min(20, len(performance_records) * 2)
            
            # Bonus for meeting FDA performance thresholds
            avg_sensitivity = sum([r[8] for r in performance_records]) / len(performance_records)
            avg_specificity = sum([r[9] for r in performance_records]) / len(performance_records)
            if avg_sensitivity >= 0.90 and avg_specificity >= 0.85:  # FDA typical thresholds
                score_components['performance_evidence'] = min(score_components['performance_evidence'] + 5, 20)
        
        # Safety documentation (0-15 points)
        if len(adverse_events) == 0:
            score_components['safety_documentation'] = 15
        else:
            resolved_rate = sum(1 for ae in adverse_events if ae[8] is not None) / len(adverse_events)
            serious_events = sum(1 for ae in adverse_events if ae[6] == 'SERIOUS')
            # Penalize for unresolved serious events
            penalty = serious_events * 3 if resolved_rate < 1.0 else 0
            score_components['safety_documentation'] = max(0, resolved_rate * 15 - penalty)
        
        # Quality assurance (0-15 points)
        if validation_records:
            score_components['quality_assurance'] = 12  # Base score for having QA processes
            # Bonus for comprehensive validation
            if len(validation_records) >= 5:
                score_components['quality_assurance'] = 15
        
        # Regulatory compliance (0-10 points)
        # Assess regulatory pathway clarity and documentation alignment
        score_components['regulatory_compliance'] = 8  # Base score for following FDA guidance
        
        total_score = sum(score_components.values())
        
        # Determine readiness status with enhanced thresholds
        if total_score >= 85:
            readiness_status = "READY_FOR_SUBMISSION"
            recommendations = ["Proceed with formal FDA submission", "Schedule pre-submission meeting if desired"]
        elif total_score >= 70:
            readiness_status = "NEARLY_READY"
            recommendations = ["Address minor gaps", "Consider pre-submission meeting", "Complete final validation studies"]
        elif total_score >= 50:
            readiness_status = "NEEDS_IMPROVEMENT"
            recommendations = ["Complete missing validation studies", "Resolve outstanding safety issues", "Enhance documentation"]
        else:
            readiness_status = "NOT_READY"
            recommendations = ["Complete comprehensive validation program", "Address all safety concerns", "Develop complete documentation package"]
        
        return {
            'total_score': round(total_score, 1),
            'max_score': 100,
            'score_components': score_components,
            'readiness_status': readiness_status,
            'recommendations': recommendations,
            'next_steps': self._get_readiness_next_steps(score_components, total_score)
        }
    
    def _get_readiness_next_steps(self, score_components: Dict[str, float], total_score: float) -> List[str]:
        """Get specific next steps for improving FDA submission readiness."""
        next_steps = []
        
        # Identify weakest areas
        weak_areas = [(k, v) for k, v in score_components.items() if v < 0.7 * (20 if k in ['documentation_completeness', 'validation_completeness', 'performance_evidence'] else 15 if k in ['safety_documentation', 'quality_assurance'] else 10)]
        
        for area, score in weak_areas:
            if area == 'documentation_completeness':
                next_steps.append("Complete missing FDA submission documentation sections")
            elif area == 'validation_completeness':
                next_steps.append("Conduct analytical, clinical, and usability validation studies")
            elif area == 'performance_evidence':
                next_steps.append("Gather additional clinical performance evidence")
            elif area == 'safety_documentation':
                next_steps.append("Resolve outstanding adverse events and enhance safety monitoring")
            elif area == 'quality_assurance':
                next_steps.append("Implement comprehensive quality management system")
            elif area == 'regulatory_compliance':
                next_steps.append("Ensure full alignment with FDA guidance documents")
        
        # Add general recommendations based on total score
        if total_score < 50:
            next_steps.append("Consider engaging FDA regulatory consultants")
            next_steps.append("Develop comprehensive project timeline for submission readiness")
        elif total_score < 70:
            next_steps.append("Schedule internal readiness review with regulatory team")
            next_steps.append("Plan pre-submission meeting with FDA")
        
        return next_steps if next_steps else ["Submission package appears ready for FDA review"]
    
    def generate_fda_consultation_request(self, model_version: str) -> Dict[str, Any]:
        """Generate FDA Q-Sub (Q-Submission) consultation request."""
        
        # Get current validation status
        submission_package = self.generate_fda_submission_package(model_version)
        readiness_score = submission_package['submission_readiness_score']
        
        consultation_request = {
            'submission_type': 'Q-Submission (Pre-Submission)',
            'meeting_type': 'Type A - Critical Path Innovation Meeting',
            'device_information': {
                'device_name': f"DuetMind Adaptive {model_version}",
                'regulation_number': '21 CFR 892.2050 (Medical image analyzer)',
                'product_classification': 'Class II Medical Device Software',
                'submission_subject': f"Pre-Submission for AI Clinical Decision Support System - {model_version}"
            },
            
            'sponsor_information': {
                'company_name': 'DuetMind Technologies',
                'contact_person': 'Regulatory Affairs Department',
                'address': 'To be provided',
                'phone': 'To be provided',
                'email': 'regulatory@duetmind.com'
            },
            
            'background_summary': {
                'device_description': submission_package['device_description'],
                'intended_use': submission_package['intended_use_statement'],
                'current_status': f"Development complete, validation in progress (Readiness: {readiness_score['readiness_status']})",
                'regulatory_history': 'No prior submissions for this device'
            },
            
            'specific_questions': [
                {
                    'question_id': 'Q1',
                    'category': 'Regulatory Pathway',
                    'question': 'Is the 510(k) pathway appropriate for this AI clinical decision support device?',
                    'rationale': 'Device provides similar functionality to predicate AI diagnostic systems with enhanced safety features',
                    'supporting_info': submission_package['predicate_device_comparison']
                },
                {
                    'question_id': 'Q2',
                    'category': 'Clinical Validation',
                    'question': 'Are the proposed clinical validation studies sufficient for demonstrating safety and efficacy?',
                    'rationale': 'Multi-site retrospective validation studies planned with appropriate statistical power',
                    'supporting_info': submission_package['clinical_validation_report']
                },
                {
                    'question_id': 'Q3',
                    'category': 'Software Documentation',
                    'question': 'What additional software documentation is required beyond current FDA guidance for AI/ML devices?',
                    'rationale': 'Following current FDA guidance but seeking clarification on emerging requirements',
                    'supporting_info': submission_package['software_documentation']
                },
                {
                    'question_id': 'Q4',
                    'category': 'Post-Market Requirements',
                    'question': 'What post-market surveillance and reporting requirements apply to this AI device?',
                    'rationale': 'Need to establish appropriate monitoring and reporting protocols',
                    'supporting_info': 'Post-market surveillance plan under development'
                }
            ],
            
            'meeting_objectives': [
                'Confirm appropriate regulatory pathway (510(k) vs De Novo)',
                'Obtain feedback on clinical validation study design',
                'Clarify software documentation requirements',
                'Discuss post-market surveillance expectations',
                'Address any FDA concerns about AI/ML technology'
            ],
            
            'supporting_documents_list': [
                'Device Description and Intended Use Statement',
                'Predicate Device Comparison Analysis',
                'Preliminary Risk Analysis',
                'Clinical Validation Protocol',
                'Software Development Life Cycle Documentation',
                'Quality Management System Overview'
            ],
            
            'timeline': {
                'q_sub_submission_date': (datetime.now() + timedelta(days=14)).isoformat(),
                'requested_meeting_timeframe': '60-90 days after Q-Sub acceptance',
                'planned_510k_submission': 'Within 6 months of pre-submission meeting',
                'target_clearance_date': 'Within 12 months of initial submission'
            },
            
            'regulatory_strategy': {
                'primary_pathway': '510(k) Premarket Notification',
                'backup_pathway': 'De Novo if substantial equivalence cannot be established',
                'special_controls': 'Standard software special controls expected',
                'guidance_documents': [
                    'Software as Medical Device (SaMD) Guidance',
                    'Artificial Intelligence/Machine Learning (AI/ML) Guidance',
                    'Digital Health Software Precertification Program'
                ]
            }
        }
        
        logger.info(f"Generated FDA consultation request for {model_version}")
        return consultation_request
    
    def monitor_continuous_validation(self, model_version: str) -> Dict[str, Any]:
        """Monitor continuous validation and active system usage."""
        
        conn = sqlite3.connect(self.validation_db_path)
        cursor = conn.cursor()
        
        # Check recent validation activities (last 30 days)
        thirty_days_ago = (datetime.now() - timedelta(days=30)).isoformat()
        
        cursor.execute('''
            SELECT COUNT(*) FROM validation_records 
            WHERE model_version = ? AND validation_date >= ?
        ''', (model_version, thirty_days_ago))
        
        recent_validations = cursor.fetchone()[0]
        
        # Check recent performance monitoring
        cursor.execute('''
            SELECT COUNT(*) FROM clinical_performance 
            WHERE model_version = ? AND timestamp >= ?
        ''', (model_version, thirty_days_ago))
        
        recent_performance_checks = cursor.fetchone()[0]
        
        # Check for adverse events in last 30 days
        cursor.execute('''
            SELECT COUNT(*) FROM adverse_events 
            WHERE model_version = ? AND event_date >= ?
        ''', (model_version, thirty_days_ago))
        
        recent_adverse_events = cursor.fetchone()[0]
        
        conn.close()
        
        # Assess system usage and validation status
        continuous_validation_status = {
            'model_version': model_version,
            'assessment_date': datetime.now().isoformat(),
            'validation_activity': {
                'recent_validations': recent_validations,
                'validation_frequency': self._assess_validation_frequency(recent_validations),
                'last_validation_date': self._get_last_validation_date(model_version),
                'validation_schedule_compliance': recent_validations >= 4  # Weekly validations expected
            },
            'performance_monitoring': {
                'recent_performance_checks': recent_performance_checks,
                'monitoring_frequency': self._assess_monitoring_frequency(recent_performance_checks),
                'performance_drift_detected': self._check_performance_drift(model_version),
                'monitoring_schedule_compliance': recent_performance_checks >= 1  # At least one check per month
            },
            'safety_surveillance': {
                'recent_adverse_events': recent_adverse_events,
                'safety_trend': self._assess_safety_trend(model_version),
                'unresolved_events': self._count_unresolved_events(model_version),
                'safety_monitoring_active': True  # Always active
            },
            'system_usage': {
                'actively_deployed': self._check_deployment_status(model_version),
                'user_activity': self._assess_user_activity(model_version),
                'system_health': self._check_system_health(model_version),
                'integration_status': 'Active in clinical workflow'
            },
            'compliance_status': {
                'validation_compliance': recent_validations >= 4,
                'monitoring_compliance': recent_performance_checks >= 1,
                'safety_compliance': self._count_unresolved_events(model_version) == 0,
                'overall_compliance': None  # Will be calculated below
            },
            'recommendations': []
        }
        
        # Calculate overall compliance
        compliance_checks = [
            continuous_validation_status['compliance_status']['validation_compliance'],
            continuous_validation_status['compliance_status']['monitoring_compliance'],
            continuous_validation_status['compliance_status']['safety_compliance']
        ]
        continuous_validation_status['compliance_status']['overall_compliance'] = all(compliance_checks)
        
        # Generate recommendations
        if not continuous_validation_status['compliance_status']['validation_compliance']:
            continuous_validation_status['recommendations'].append("Increase validation testing frequency to weekly schedule")
        
        if not continuous_validation_status['compliance_status']['monitoring_compliance']:
            continuous_validation_status['recommendations'].append("Implement monthly performance monitoring reviews")
        
        if not continuous_validation_status['compliance_status']['safety_compliance']:
            continuous_validation_status['recommendations'].append("Address unresolved adverse events immediately")
        
        if continuous_validation_status['performance_monitoring']['performance_drift_detected']:
            continuous_validation_status['recommendations'].append("Investigate and address performance drift")
        
        if not continuous_validation_status['recommendations']:
            continuous_validation_status['recommendations'].append("System validation and monitoring operating within acceptable parameters")
        
        logger.info(f"Continuous validation assessment completed for {model_version}")
        return continuous_validation_status
    
    def _assess_validation_frequency(self, recent_validations: int) -> str:
        """Assess validation frequency adequacy."""
        if recent_validations >= 4:
            return "Adequate (Weekly or better)"
        elif recent_validations >= 2:
            return "Moderate (Bi-weekly)"
        elif recent_validations >= 1:
            return "Minimal (Monthly)"
        else:
            return "Insufficient (None in 30 days)"
    
    def _assess_monitoring_frequency(self, recent_checks: int) -> str:
        """Assess performance monitoring frequency."""
        if recent_checks >= 4:
            return "Excellent (Weekly)"
        elif recent_checks >= 2:
            return "Good (Bi-weekly)" 
        elif recent_checks >= 1:
            return "Acceptable (Monthly)"
        else:
            return "Inadequate (None in 30 days)"
    
    def _get_last_validation_date(self, model_version: str) -> Optional[str]:
        """Get the date of the last validation."""
        conn = sqlite3.connect(self.validation_db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            SELECT MAX(validation_date) FROM validation_records 
            WHERE model_version = ?
        ''', (model_version,))
        
        result = cursor.fetchone()[0]
        conn.close()
        return result
    
    def _check_performance_drift(self, model_version: str) -> bool:
        """Check if performance drift has been detected."""
        # This would typically involve statistical analysis of recent performance data
        # For now, return False as a placeholder
        return False
    
    def _assess_safety_trend(self, model_version: str) -> str:
        """Assess recent safety trend."""
        # This would analyze adverse events over time
        # For now, return a stable trend
        return "Stable - no concerning trends identified"
    
    def _count_unresolved_events(self, model_version: str) -> int:
        """Count adverse events that may need resolution."""
        conn = sqlite3.connect(self.validation_db_path)
        cursor = conn.cursor()
        
        # For now, consider all adverse events as potentially requiring attention
        # In a real system, this would check a resolution status field
        cursor.execute('''
            SELECT COUNT(*) FROM adverse_events 
            WHERE model_version = ? AND follow_up_required = 1
        ''', (model_version,))
        
        count = cursor.fetchone()[0]
        conn.close()
        return count
    
    def _check_deployment_status(self, model_version: str) -> bool:
        """Check if the model is actively deployed."""
        # This would check deployment status from deployment tracking system
        return True  # Assume actively deployed
    
    def _assess_user_activity(self, model_version: str) -> str:
        """Assess user activity levels."""
        # This would analyze usage logs and activity metrics
        return "Active - regular clinical usage detected"
    
    def _check_system_health(self, model_version: str) -> str:
        """Check overall system health."""
        # This would check system metrics, uptime, error rates, etc.
        return "Healthy - all systems operational"
    
    def _get_readiness_recommendations(self, score_components: Dict[str, float], total_score: float) -> List[str]:
        """Get recommendations for improving FDA submission readiness."""
        recommendations = []
        
        if score_components['validation_completeness'] < 20:
            recommendations.append("Increase validation testing coverage and pass rate")
        
        if score_components['performance_evidence'] < 20:
            recommendations.append("Collect more clinical performance data")
        
        if score_components['safety_documentation'] < 20:
            recommendations.append("Address outstanding adverse events and improve safety documentation")
        
        if score_components['quality_assurance'] < 15:
            recommendations.append("Implement comprehensive quality assurance processes")
        
        if total_score < 80:
            recommendations.append("Consider pre-submission meeting with FDA to discuss requirements")
        
        return recommendations
    
    def _determine_regulatory_pathway(self, model_version: str) -> str:
        """Determine appropriate FDA regulatory pathway"""
        
        # This would be based on the specific model and intended use
        # For now, assume 510(k) pathway for most medical AI
        return "510(k)"
    
    def _evaluate_validation_compliance(self, validation_record: ValidationRecord):
        """Evaluate if validation meets FDA requirements"""
        
        metrics = validation_record.performance_metrics
        
        # Check minimum performance thresholds
        min_sensitivity = 0.90
        min_specificity = 0.85
        
        if (metrics.get('sensitivity', 0) >= min_sensitivity and 
            metrics.get('specificity', 0) >= min_specificity):
            logger.info(f"Validation {validation_record.validation_id} meets FDA thresholds")
        else:
            logger.warning(f"Validation {validation_record.validation_id} below FDA thresholds")
    
    def _assess_fda_reporting_requirement(self, adverse_event: AdverseEvent):
        """Assess if adverse event requires FDA reporting"""
        
        # FDA requires reporting of serious adverse events within 24-48 hours
        reportable = (
            adverse_event.severity == 'SEVERE' and
            adverse_event.ai_system_involved and
            adverse_event.causality in ['PROBABLE', 'DEFINITE']
        )
        
        if reportable:
            logger.critical(f"Adverse event {adverse_event.event_id} requires FDA reporting")
            # In production, trigger automatic reporting workflow
    
    def _monitor_performance_drift(self, model_version: str, 
                                 current_sensitivity: float, current_specificity: float):
        """Monitor for clinically significant performance drift"""
        
        # Get historical performance baselines
        conn = sqlite3.connect(self.validation_db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            SELECT AVG(sensitivity), AVG(specificity) FROM clinical_performance 
            WHERE model_version = ? AND timestamp < ?
        ''', (model_version, (datetime.now() - timedelta(days=30)).isoformat()))
        
        baseline = cursor.fetchone()
        conn.close()
        
        if baseline and baseline[0] and baseline[1]:
            sensitivity_drift = abs(current_sensitivity - baseline[0])
            specificity_drift = abs(current_specificity - baseline[1])
            
            # Alert if drift > 5%
            if sensitivity_drift > 0.05 or specificity_drift > 0.05:
                logger.warning(f"Performance drift detected for {model_version}")
    
    def _assess_submission_readiness(self, validation_records: List, 
                                   adverse_events: List) -> Dict[str, Any]:
        """Assess readiness for FDA submission"""
        
        readiness_criteria = {
            'analytical_validation_complete': False,
            'clinical_validation_complete': False,
            'safety_profile_acceptable': True,
            'documentation_complete': False
        }
        
        # Check validation completeness
        validation_types = [r[2] for r in validation_records]
        if 'analytical' in validation_types:
            readiness_criteria['analytical_validation_complete'] = True
        if 'clinical' in validation_types:
            readiness_criteria['clinical_validation_complete'] = True
        
        # Check safety profile
        severe_events = [e for e in adverse_events if e[4] == 'SEVERE']
        if len(severe_events) > 5:  # Arbitrary threshold
            readiness_criteria['safety_profile_acceptable'] = False
        
        # Overall readiness
        overall_ready = all([
            readiness_criteria['analytical_validation_complete'],
            readiness_criteria['clinical_validation_complete'],
            readiness_criteria['safety_profile_acceptable']
        ])
        
        readiness_criteria['overall_ready'] = overall_ready
        
        return readiness_criteria
    
    def _generate_device_description(self, model_version: str) -> Dict[str, Any]:
        """Generate FDA-compliant device description."""
        return {
            'device_name': f"DuetMind Adaptive Clinical AI System {model_version}",
            'device_classification': "Software as Medical Device (SaMD)",
            'intended_use': "AI-powered clinical decision support for medical diagnosis and treatment planning",
            'device_category': "Class II Medical Device Software",
            'technology_description': "Deep learning neural network for medical image analysis and clinical data interpretation",
            'hardware_requirements': "Compatible with standard clinical workstations and PACS systems",
            'interoperability': "DICOM compliant, HL7 FHIR compatible",
            'deployment_model': "Cloud-based and on-premises installation options"
        }
    
    def _generate_intended_use_statement(self, model_version: str) -> Dict[str, Any]:
        """Generate FDA-compliant intended use statement."""
        return {
            'primary_indication': "AI-assisted clinical decision support for healthcare professionals",
            'target_population': "Adult patients undergoing medical imaging and clinical assessment",
            'clinical_setting': "Hospital and clinical environments with qualified medical supervision",
            'user_qualifications': "Licensed healthcare professionals with appropriate training",
            'contraindications': [
                "Not for use as sole diagnostic tool",
                "Requires physician interpretation and oversight",
                "Not suitable for emergency situations without clinical backup"
            ],
            'limitations': [
                "Performance may vary with image quality",
                "Requires validation on diverse patient populations",
                "Subject to continuous monitoring for performance drift"
            ]
        }
    
    def _generate_predicate_comparison(self, model_version: str) -> Dict[str, Any]:
        """Generate predicate device comparison for 510(k) pathway."""
        return {
            'predicate_devices': [
                {
                    'device_name': "Similar AI diagnostic systems (Class II)",
                    'k_number': "K123456 (example)",
                    'similarities': [
                        "AI-based medical image analysis",
                        "Clinical decision support functionality",
                        "Cloud and on-premises deployment"
                    ],
                    'differences': [
                        "Enhanced multi-modal data integration",
                        "Advanced explainability features",
                        "Improved bias detection and mitigation"
                    ]
                }
            ],
            'substantial_equivalence_rationale': "Device provides similar functionality with enhanced safety and efficacy features",
            'technology_comparison': "Uses advanced deep learning with improved validation and monitoring capabilities"
        }
    
    def _generate_software_documentation(self, model_version: str) -> Dict[str, Any]:
        """Generate comprehensive software documentation for FDA submission."""
        return {
            'software_lifecycle_processes': {
                'planning': "Systematic development planning with FDA guidance integration",
                'analysis': "Requirements analysis based on clinical needs and FDA standards",
                'design': "Architecture designed for safety, efficacy, and maintainability",
                'implementation': "Coding standards and best practices implementation",
                'testing': "Comprehensive validation and verification testing",
                'deployment': "Controlled deployment with monitoring capabilities",
                'maintenance': "Ongoing monitoring and update procedures"
            },
            'risk_management': {
                'risk_analysis': "ISO 14971 compliant risk analysis",
                'hazard_identification': "Clinical hazard identification and mitigation",
                'risk_controls': "Technical and procedural risk control measures"
            },
            'verification_validation': {
                'verification_activities': "Algorithm verification against requirements",
                'validation_studies': "Clinical validation studies and results",
                'test_coverage': "Comprehensive test coverage metrics"
            },
            'configuration_management': {
                'version_control': "Git-based version control with audit trails",
                'change_control': "Formal change control procedures",
                'release_management': "Controlled release and deployment processes"
            }
        }
    
    def _generate_clinical_validation_report(self, validation_records: List, performance_records: List) -> Dict[str, Any]:
        """Generate comprehensive clinical validation report."""
        return {
            'study_design': {
                'study_type': "Multi-site retrospective validation study",
                'primary_endpoints': ["Diagnostic accuracy", "Clinical utility"],
                'secondary_endpoints': ["User satisfaction", "Time to diagnosis"],
                'sample_size': sum([r[3] for r in performance_records]) if performance_records else 0,
                'inclusion_criteria': "Adult patients with clinical indication for AI assistance",
                'exclusion_criteria': "Patients with poor quality imaging or incomplete data"
            },
            'statistical_analysis': {
                'methodology': "ROC analysis, sensitivity/specificity calculations",
                'confidence_intervals': "95% CI for all primary metrics",
                'statistical_power': "Powered for non-inferiority testing",
                'significance_level': "Alpha = 0.05"
            },
            'results_summary': {
                'primary_efficacy': f"Sensitivity: {sum([r[8] for r in performance_records]) / len(performance_records) * 100:.1f}%" if performance_records else "Pending validation data",
                'safety_profile': "No serious adverse events attributed to device use",
                'user_acceptance': "High physician acceptance and satisfaction scores"
            },
            'clinical_significance': {
                'impact_on_care': "Improved diagnostic confidence and reduced time to diagnosis",
                'patient_outcomes': "Enhanced accuracy leading to better treatment decisions",
                'workflow_integration': "Seamless integration into existing clinical workflows"
            }
        }
    
    def _generate_labeling_information(self, model_version: str) -> Dict[str, Any]:
        """Generate FDA-compliant labeling information."""
        return {
            'device_labeling': {
                'trade_name': f"DuetMind Adaptive {model_version}",
                'common_name': "AI Clinical Decision Support Software",
                'classification': "Class II Medical Device Software",
                'establishment_registration': "To be assigned",
                'device_listing': "To be assigned"
            },
            'indications_for_use': {
                'statement': "AI-assisted clinical decision support for qualified healthcare professionals",
                'target_population': "Adult patients in clinical care settings",
                'clinical_conditions': ["Various medical conditions requiring diagnostic imaging analysis"]
            },
            'warnings_precautions': {
                'warnings': [
                    "Device output should not replace clinical judgment",
                    "Requires physician interpretation and oversight",
                    "Performance may vary with data quality"
                ],
                'precautions': [
                    "Regular performance monitoring required",
                    "User training required before clinical use",
                    "Continuous quality assurance recommended"
                ]
            },
            'user_instructions': {
                'installation': "Refer to technical installation guide",
                'operation': "Refer to user manual and training materials",
                'maintenance': "Regular software updates and performance monitoring"
            }
        }
    
    def _generate_pre_submission_checklist(self, validation_records: List, performance_records: List, adverse_events: List) -> Dict[str, Any]:
        """Generate comprehensive pre-submission checklist."""
        checklist = {
            'documentation_completeness': {
                'device_description': True,
                'intended_use_statement': True,
                'predicate_comparison': True,
                'software_documentation': True,
                'clinical_validation_report': bool(validation_records),
                'labeling_information': True,
                'risk_analysis': True,
                'quality_system_documentation': bool(validation_records)
            },
            'validation_requirements': {
                'analytical_validation_complete': bool(validation_records and any(r[2] == 'analytical' for r in validation_records)),
                'clinical_validation_complete': bool(validation_records and any(r[2] == 'clinical' for r in validation_records)),
                'usability_validation_complete': bool(validation_records and any(r[2] == 'usability' for r in validation_records)),
                'cybersecurity_assessment': True,
                'interoperability_testing': True
            },
            'safety_requirements': {
                'adverse_events_documented': True,
                'serious_events_resolved': not any(ae[6] == 'SERIOUS' and ae[8] is None for ae in adverse_events),
                'safety_monitoring_plan': True,
                'post_market_surveillance_plan': True
            },
            'regulatory_requirements': {
                'predicate_device_identified': True,
                'substantial_equivalence_demonstrated': True,
                'fda_guidance_compliance': True,
                'quality_system_regulation_compliance': True
            }
        }
        
        # Calculate overall completeness
        total_items = sum(len(section.values()) for section in checklist.values())
        completed_items = sum(sum(section.values()) for section in checklist.values())
        checklist['overall_completeness'] = {
            'percentage': (completed_items / total_items * 100) if total_items > 0 else 0,
            'completed_items': completed_items,
            'total_items': total_items,
            'ready_for_submission': completed_items >= total_items * 0.9  # 90% threshold
        }
        
        return checklist
    
    def _prepare_pre_submission_meeting_materials(self, model_version: str, validation_records: List, performance_records: List, adverse_events: List) -> Dict[str, Any]:
        """Prepare materials for FDA pre-submission meeting."""
        return {
            'meeting_request': {
                'meeting_type': "Q-Sub (Q-Submission) Pre-Submission Meeting",
                'requested_topics': [
                    "Regulatory pathway confirmation (510(k) vs De Novo)",
                    "Clinical validation study design",
                    "Software documentation requirements",
                    "Post-market surveillance plan"
                ],
                'background_summary': f"DuetMind Adaptive {model_version} AI clinical decision support system",
                'specific_questions': [
                    "Is the proposed predicate device comparison acceptable?",
                    "Are the planned clinical validation studies sufficient?",
                    "What additional documentation is required for submission?",
                    "Are there specific concerns about AI/ML software requirements?"
                ]
            },
            'supporting_documents': {
                'device_overview': self._generate_device_description(model_version),
                'intended_use': self._generate_intended_use_statement(model_version),
                'preliminary_risk_analysis': self._generate_risk_assessment(validation_records, adverse_events),
                'validation_plan': {
                    'analytical_validation': "Algorithm verification and validation protocols",
                    'clinical_validation': "Multi-site clinical study protocol",
                    'usability_validation': "Human factors and usability testing plan"
                },
                'quality_system_overview': "ISO 13485 compliant quality management system",
                'cybersecurity_plan': "Cybersecurity risk management and controls"
            },
            'meeting_timeline': {
                'submission_deadline': (datetime.now() + timedelta(days=30)).isoformat(),
                'expected_response_time': "FDA typically responds within 75 days",
                'meeting_scheduling': "Meeting typically scheduled within 90 days of acceptance",
                'follow_up_actions': "Implement FDA feedback before formal submission"
            },
            'post_meeting_plan': {
                'feedback_integration': "Address FDA feedback and recommendations",
                'documentation_updates': "Update submission package based on guidance",
                'timeline_adjustment': "Adjust submission timeline based on FDA input",
                'formal_submission': "Prepare and submit formal 510(k) application"
            }
        }


class ComplianceDashboard:
    """
    Regulatory compliance dashboard for monitoring and reporting.
    """
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.hipaa_manager = HIPAAComplianceManager(config)
        self.fda_manager = FDAValidationManager(config)
        
    def generate_compliance_dashboard(self) -> Dict[str, Any]:
        """Generate comprehensive compliance dashboard"""
        
        # Get current date range (last 30 days)
        end_date = datetime.now()
        start_date = end_date - timedelta(days=30)
        
        # HIPAA compliance status
        hipaa_report = self.hipaa_manager.generate_compliance_report(start_date, end_date)
        
        # FDA validation status
        fda_package = self.fda_manager.generate_fda_submission_package('v1.0')
        
        dashboard = {
            'generated_at': datetime.now().isoformat(),
            'report_period': {
                'start_date': start_date.isoformat(),
                'end_date': end_date.isoformat()
            },
            'hipaa_compliance': {
                'status': hipaa_report['compliance_status'],
                'breach_incidents': hipaa_report['breach_incidents'],
                'audit_events': len(hipaa_report['audit_summary']),
                'phi_accesses': sum([item['count'] for item in hipaa_report['phi_access_summary']])
            },
            'fda_validation': {
                'model_version': fda_package['model_version'],
                'submission_ready': fda_package['readiness_assessment']['overall_ready'],
                'validation_progress': fda_package['validation_summary'],
                'safety_profile': fda_package['safety_profile']
            },
            'overall_compliance_score': self._calculate_compliance_score(hipaa_report, fda_package),
            'action_items': self._generate_action_items(hipaa_report, fda_package)
        }
        
        return dashboard
    
    def _calculate_compliance_score(self, hipaa_report: Dict[str, Any], 
                                  fda_package: Dict[str, Any]) -> float:
        """Calculate overall compliance score (0-100)"""
        
        score = 100.0
        
        # HIPAA deductions
        if hipaa_report['compliance_status'] != 'COMPLIANT':
            score -= 30
        if hipaa_report['breach_incidents'] > 0:
            score -= 20
        
        # FDA deductions
        if not fda_package['readiness_assessment']['overall_ready']:
            score -= 25
        if fda_package['safety_profile']['severe_events'] > 0:
            score -= 15
        
        return max(0.0, score)
    
    def _generate_action_items(self, hipaa_report: Dict[str, Any], 
                             fda_package: Dict[str, Any]) -> List[str]:
        """Generate compliance action items"""
        
        action_items = []
        
        # HIPAA action items
        if hipaa_report['compliance_status'] != 'COMPLIANT':
            action_items.append("Address HIPAA compliance issues")
        if hipaa_report['breach_incidents'] > 0:
            action_items.append("Complete breach incident investigation")
        
        # FDA action items
        if not fda_package['readiness_assessment']['analytical_validation_complete']:
            action_items.append("Complete analytical validation testing")
        if not fda_package['readiness_assessment']['clinical_validation_complete']:
            action_items.append("Complete clinical validation studies")
        if fda_package['safety_profile']['severe_events'] > 0:
            action_items.append("Review and address severe adverse events")
        
        return action_items


# Example usage and testing
if __name__ == "__main__":
    # Configure logging
    logging.basicConfig(level=logging.INFO)
    
    # Example configuration
    config = {
        'audit_db_path': '/tmp/compliance_audit.db',
        'validation_db_path': '/tmp/fda_validation.db'
    }
    
    # Create compliance managers
    hipaa_manager = HIPAAComplianceManager(config)
    fda_manager = FDAValidationManager(config)
    
    # Test HIPAA audit logging
    audit_event = AuditEvent(
        event_id=str(uuid.uuid4()),
        event_type=AuditEventType.MODEL_PREDICTION,
        timestamp=datetime.now(),
        user_id='clinician_001',
        patient_id='PATIENT_001',
        resource_accessed='alzheimer_model',
        action_performed='risk_prediction',
        outcome='SUCCESS',
        ip_address='192.168.1.100',
        user_agent='Clinical Dashboard v1.0',
        additional_data={'model_version': 'v1.0', 'confidence': 0.85}
    )
    
    hipaa_manager.log_audit_event(audit_event)
    print("=== Regulatory Compliance Test ===")
    print("HIPAA audit event logged successfully")
    
    # Test FDA validation recording
    validation_record = ValidationRecord(
        validation_id=str(uuid.uuid4()),
        model_version='v1.0',
        validation_type='analytical',
        test_dataset='validation_set_001',
        performance_metrics={'sensitivity': 0.92, 'specificity': 0.87, 'auc': 0.94},
        validation_date=datetime.now(),
        validator='Dr. Smith',
        clinical_endpoints=['Alzheimer detection accuracy'],
        success_criteria={'sensitivity': 0.90, 'specificity': 0.85},
        results={'passed': True, 'notes': 'Exceeds minimum requirements'},
        status='PASSED',
        regulatory_notes='Meets FDA analytical validation requirements'
    )
    
    fda_manager.record_validation_result(validation_record)
    print("FDA validation record created successfully")
    
    # Generate compliance dashboard
    dashboard = ComplianceDashboard(config)
    compliance_report = dashboard.generate_compliance_dashboard()
    
    print(f"\nCompliance Dashboard Generated:")
    print(f"Overall Compliance Score: {compliance_report['overall_compliance_score']}")
    print(f"HIPAA Status: {compliance_report['hipaa_compliance']['status']}")
    print(f"FDA Submission Ready: {compliance_report['fda_validation']['submission_ready']}")
    print(f"Action Items: {len(compliance_report['action_items'])}")