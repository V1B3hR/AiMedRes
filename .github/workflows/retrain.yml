name: MLOps Retraining Pipeline

on:
  push:
    branches: [ main ]
    paths: 
      - 'mlops/**'
      - 'params.yaml'
      - 'dvc.yaml'
  schedule:
    # Run weekly on Sundays at 2 AM UTC
    - cron: '0 2 * * 0'
  workflow_dispatch:
    inputs:
      force_retrain:
        description: 'Force retraining even if no improvements'
        required: false
        default: 'false'
        type: boolean

env:
  PYTHON_VERSION: '3.11'
  DVC_CACHE_TYPE: 'symlink'

jobs:
  validate-and-retrain:
    runs-on: ubuntu-latest
    
    services:
      postgres:
        image: postgres:15-alpine
        env:
          POSTGRES_DB: duetmind_mlops_ci
          POSTGRES_USER: duetmind_ci
          POSTGRES_PASSWORD: test_password
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Full history for DVC

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements*.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install mlflow dvc pandera alembic psycopg2-binary

    - name: Setup MLOps environment
      run: |
        # Create test environment configuration
        cat > .env << EOF
        DB_HOST=localhost
        DB_PORT=5432
        DB_NAME=duetmind_mlops_ci
        DB_USER=duetmind_ci
        DB_PASSWORD=test_password
        MLFLOW_TRACKING_URI=sqlite:///mlflow_ci.db
        ENVIRONMENT=ci
        EOF

    - name: Setup database schema
      run: |
        # Update params for CI environment
        sed -i 's/duetmind_mlops/duetmind_mlops_ci/g' params.yaml
        sed -i 's/duetmind:duetmind_secret/duetmind_ci:test_password/g' params.yaml
        
        # Run database migrations
        cd mlops/infra
        alembic upgrade head

    - name: Validate data schemas
      run: |
        # Run schema validation if data exists
        if [ -f data/raw/alzheimer_sample.csv ]; then
          python mlops/validation/schema_contracts.py data/raw/alzheimer_sample.csv --schema raw
        fi
        
        echo "Schema validation completed"

    - name: Check for data drift
      id: drift_check
      run: |
        # Create minimal drift check for CI
        python -c "
        import pandas as pd
        import os
        import yaml
        
        # Load params
        with open('params.yaml', 'r') as f:
            params = yaml.safe_load(f)
        
        # Simple drift simulation for CI
        print('Running drift detection...')
        drift_detected = False  # Simulate no drift for CI
        
        if drift_detected:
            print('::set-output name=drift_detected::true')
            print('Significant drift detected - retraining required')
        else:
            print('::set-output name=drift_detected::false')
            print('No significant drift detected')
        "

    - name: Run ML pipeline
      id: training
      run: |
        # Set MLflow tracking URI for CI
        export MLFLOW_TRACKING_URI=sqlite:///mlflow_ci.db
        
        # Run the full pipeline
        echo "Starting ML pipeline..."
        
        # Ingest data
        python mlops/pipelines/ingest_raw.py
        
        # Build features and validate schema
        python mlops/pipelines/build_features.py
        
        # Validate processed data
        python mlops/validation/schema_contracts.py data/processed/features.parquet --schema features
        python mlops/validation/schema_contracts.py data/processed/labels.parquet --schema labels
        
        # Train model
        python mlops/pipelines/train_model.py
        
        echo "ML pipeline completed successfully"

    - name: Evaluate model performance
      id: evaluation
      run: |
        python -c "
        import mlflow
        import json
        import os
        
        # Set MLflow tracking URI
        mlflow.set_tracking_uri('sqlite:///mlflow_ci.db')
        
        # Get latest run
        experiment = mlflow.get_experiment_by_name('duetmind_alzheimer_prediction')
        if experiment:
            runs = mlflow.search_runs(experiment_ids=[experiment.experiment_id])
            if not runs.empty:
                latest_run = runs.iloc[0]
                
                # Extract metrics
                accuracy = latest_run.get('metrics.accuracy', 0)
                roc_auc = latest_run.get('metrics.roc_auc', 0)
                
                print(f'Model performance: Accuracy={accuracy:.4f}, ROC_AUC={roc_auc:.4f}')
                
                # Check if model meets thresholds
                min_accuracy = 0.75  # Lower threshold for CI
                min_roc_auc = 0.70
                
                meets_threshold = accuracy >= min_accuracy and roc_auc >= min_roc_auc
                
                if meets_threshold:
                    print('‚úÖ Model meets performance thresholds')
                    print(f'::set-output name=model_approved::true')
                else:
                    print('‚ùå Model does not meet performance thresholds')
                    print(f'::set-output name=model_approved::false')
                    
                # Save metrics
                metrics = {
                    'accuracy': accuracy,
                    'roc_auc': roc_auc,
                    'meets_threshold': meets_threshold,
                    'run_id': latest_run['run_id']
                }
                
                with open('model_metrics.json', 'w') as f:
                    json.dump(metrics, f)
            else:
                print('No MLflow runs found')
                print(f'::set-output name=model_approved::false')
        else:
            print('MLflow experiment not found')
            print(f'::set-output name=model_approved::false')
        "

    - name: Update model registry
      if: steps.evaluation.outputs.model_approved == 'true' || github.event.inputs.force_retrain == 'true'
      run: |
        echo "Updating model registry..."
        
        # In a real deployment, this would:
        # 1. Register model in production registry
        # 2. Update model serving endpoint
        # 3. Run A/B testing setup
        # 4. Update monitoring dashboards
        
        echo "Model registry update completed"

    - name: Run post-training validation
      run: |
        # Validate that all artifacts were created
        echo "Validating training artifacts..."
        
        required_files=(
          "data/processed/features.parquet"
          "data/processed/labels.parquet"
          "mlflow_ci.db"
        )
        
        for file in "${required_files[@]}"; do
          if [ -f "$file" ]; then
            echo "‚úÖ $file exists"
          else
            echo "‚ùå $file missing"
            exit 1
          fi
        done
        
        echo "All required artifacts validated"

    - name: Generate training report
      run: |
        python -c "
        import json
        import os
        from datetime import datetime
        
        # Load metrics if available
        metrics = {}
        if os.path.exists('model_metrics.json'):
            with open('model_metrics.json', 'r') as f:
                metrics = json.load(f)
        
        # Generate report
        report = {
            'timestamp': datetime.now().isoformat(),
            'pipeline_status': 'completed',
            'trigger': '${{ github.event_name }}',
            'commit_sha': '${{ github.sha }}',
            'metrics': metrics,
            'artifacts_created': True
        }
        
        print('Training Report:')
        print(json.dumps(report, indent=2))
        
        # Save report
        with open('training_report.json', 'w') as f:
            json.dump(report, f, indent=2)
        "

    - name: Archive training artifacts
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: training-artifacts
        path: |
          training_report.json
          model_metrics.json
          mlflow_ci.db
        retention-days: 30

    - name: Notify on failure
      if: failure()
      run: |
        echo "üö® Training pipeline failed!"
        echo "Check the logs for details and fix any issues."
        # In production, this would send alerts via Slack/email/PagerDuty

    - name: Notify on success
      if: success()
      run: |
        echo "‚úÖ Training pipeline completed successfully!"
        echo "Model artifacts have been generated and validated."